{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First our imports.  numpy and tensorflow are the matrix and neural net packages (Keras is a 'fast to develop' api of tensorflow).  In tf.Keras the imdb dataset is pre-cleaned allowing us to focus on Neural Networks.\n",
    "\n",
    "THe Sequential is just how we build the model (sequentially, adding layers).  We'll import the RNN, LSTM, and GRU cells as well as a special layer known as 'Embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import  LSTM as LSTM, SimpleRNN,  GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pull the top words from the dataset.  Everything not in this dataset is coded as 'unknown' Because the imdb dataset is sorted, it makes the data faster to load.  In addition, we create test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make all our data the same length.  We set the maximum review length to be 500 and anything shorter than 500 tokens is padded with 0's at the beginning. Padding is at the beginning in RNNs as feeding 0s late in the cycle may cause memory loss (think about bi-directional challenges though!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a simple RNN model.  The embedding layer allows us to train our own word vectors.  It becomes in essence a lookup table for the word vectors as the first word \"the\" is encoded as \"4\"  That in essence means \"Grab the 4th row of the embedding column as inputs\"  Also because this is a weight matrix the weights will be learned.  (In fine detail the \"4\" becomes one hot encoded as a vectors with 0s everywhere except position 4, and multiplied by the weight matrix which 'selects' row 4.  However it is more effcient in implmentation to just make a lookup table than store a large matrix and multiply.\n",
    "\n",
    "Ask yourself why are the number of parameters the way they are?  Can you calculate how many there should be?  Does it match? (Hint: It should!)\n",
    "\n",
    "Notice we are validating are the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 80)           400000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               18100     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 418,201\n",
      "Trainable params: 418,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 265s 11ms/sample - loss: 0.6461 - acc: 0.6309 - val_loss: 0.5161 - val_acc: 0.7514\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 374s 15ms/sample - loss: 0.5626 - acc: 0.7034 - val_loss: 0.4614 - val_acc: 0.7935\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 491s 20ms/sample - loss: 0.4384 - acc: 0.7965 - val_loss: 0.4969 - val_acc: 0.7593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e087668470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 80\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model1.add(SimpleRNN(100,unroll=True))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let check how we performed on the test data overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.93%\n"
     ]
    }
   ],
   "source": [
    "scores = model1.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can swap out our RNN for an LSTM.  Keeping everything the same, how improved is it? (if at all)?  Can you make sense of the number of parameters here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 80)           400000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               72400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 472,501\n",
      "Trainable params: 472,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 34s 1ms/sample - loss: 0.4711 - accuracy: 0.7739 - val_loss: 0.3440 - val_accuracy: 0.8548\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.4751 - accuracy: 0.7692 - val_loss: 0.6146 - val_accuracy: 0.6598\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.4439 - accuracy: 0.7926 - val_loss: 0.3649 - val_accuracy: 0.8475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x158d34f6828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 80\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.75%\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use a GRU cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 80)           400000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               54600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 454,701\n",
      "Trainable params: 454,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.5297 - accuracy: 0.7387 - val_loss: 0.3702 - val_accuracy: 0.8428\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 28s 1ms/sample - loss: 0.3084 - accuracy: 0.8731 - val_loss: 0.3772 - val_accuracy: 0.8424\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 28s 1ms/sample - loss: 0.2610 - accuracy: 0.8970 - val_loss: 0.3389 - val_accuracy: 0.8639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15703d30cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 80\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model3.add(GRU(100))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n",
    "model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.39%\n"
     ]
    }
   ],
   "source": [
    "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of training our own vectors let's try pre-trained GloVe vectors.  First I will read them in.  You must download them from the Glove website.  Why didn't I use a length 80 vector?  because Glove vectors are only downloadable in certain sizes--in this case 100 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt','r',encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we continue, let's take a look at what the data really was.  I like to tinker around in the data and make sure I understand what is happening. Its a good way to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=5000  # only use top 1000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "train,test = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "train_x,train_y = train\n",
    "test_x,test_y = test\n",
    "\n",
    "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in train_x[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I do here is replace the embedding matrix with the word vectors.  So when the embedding layer for the word 'the' comes up, my 100 length vector with 0's everywhere except position 4 will select the 4th rows of the embedding matrix, which we put in the glove vector for 'the'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size=5000\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "index=0\n",
    "for word in word_to_id:\n",
    "    index = word_to_id[word]\n",
    "    if index > vocabulary_size - 1:\n",
    "        pass\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "embedding_matrix[2] = embeddings_index.get('unk')\n",
    "embedding_matrix[1] = np.ones(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['film']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check that GloVe and embedding matrix match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194  , -0.24487001,  0.72812003, -0.39961001,  0.083172  ,\n",
       "        0.043953  , -0.39140999,  0.3344    , -0.57545   ,  0.087459  ,\n",
       "        0.28786999, -0.06731   ,  0.30906001, -0.26383999, -0.13231   ,\n",
       "       -0.20757   ,  0.33395001, -0.33848   , -0.31742999, -0.48335999,\n",
       "        0.1464    , -0.37303999,  0.34577   ,  0.052041  ,  0.44946   ,\n",
       "       -0.46970999,  0.02628   , -0.54154998, -0.15518001, -0.14106999,\n",
       "       -0.039722  ,  0.28277001,  0.14393   ,  0.23464   , -0.31020999,\n",
       "        0.086173  ,  0.20397   ,  0.52623999,  0.17163999, -0.082378  ,\n",
       "       -0.71787   , -0.41531   ,  0.20334999, -0.12763   ,  0.41367   ,\n",
       "        0.55186999,  0.57907999, -0.33476999, -0.36559001, -0.54856998,\n",
       "       -0.062892  ,  0.26583999,  0.30204999,  0.99774998, -0.80480999,\n",
       "       -3.0243001 ,  0.01254   , -0.36941999,  2.21670008,  0.72201002,\n",
       "       -0.24978   ,  0.92136002,  0.034514  ,  0.46744999,  1.10790002,\n",
       "       -0.19358   , -0.074575  ,  0.23353   , -0.052062  , -0.22044   ,\n",
       "        0.057162  , -0.15806   , -0.30798   , -0.41624999,  0.37972   ,\n",
       "        0.15006   , -0.53211999, -0.20550001, -1.25259995,  0.071624  ,\n",
       "        0.70564997,  0.49744001, -0.42063001,  0.26148   , -1.53799999,\n",
       "       -0.30223   , -0.073438  , -0.28312001,  0.37103999, -0.25217   ,\n",
       "        0.016215  , -0.017099  , -0.38984001,  0.87423998, -0.72569001,\n",
       "       -0.51058   , -0.52028   , -0.1459    ,  0.82779998,  0.27061999])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I will redo the simple RNN, notice the change to the embedding layer where I tell it the weights and set the layer to be frozen of 'trainable=False'.  That means do not do weight calculations for our embedding matrix.  Notice how I also resized the Embedding layer-> top_words+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 520,201\n",
      "Trainable params: 20,201\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.6514 - accuracy: 0.6118 - val_loss: 0.6309 - val_accuracy: 0.6357\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 45s 2ms/sample - loss: 0.5897 - accuracy: 0.6905 - val_loss: 0.5844 - val_accuracy: 0.7032\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 46s 2ms/sample - loss: 0.5799 - accuracy: 0.6986 - val_loss: 0.5725 - val_accuracy: 0.7112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15959e189e8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 100\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(top_words, embedding_vecor_length, weights=[embedding_matrix], input_length=max_review_length, trainable=False))\n",
    "model4.add(SimpleRNN(100,unroll=True))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model4.summary())\n",
    "model4.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.12%\n"
     ]
    }
   ],
   "source": [
    "scores = model4.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a model with trainable weights here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.93%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "second_layer_weights = model4.layers[0].get_weights()[0] #this gets your word vectors from the model--layer 0 is the embedding layer\n",
    "scipy.spatial.distance.cosine(second_layer_weights[22],embeddings_index['film']) #don't forget, scipy uses the 1- cos x version!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19916 , -0.049702,  0.24579 , -0.32281 ,  0.89768 , -0.1278  ,\n",
       "       -0.49506 ,  0.20814 , -0.20046 , -0.20604 ,  0.038292, -0.67277 ,\n",
       "       -0.12689 , -0.18766 , -0.10277 ,  0.73128 ,  0.82408 ,  0.087288,\n",
       "        0.69255 ,  1.3107  ,  0.49113 , -0.38097 ,  0.24338 , -0.27813 ,\n",
       "        0.62506 ,  0.35978 ,  0.42041 , -0.24529 ,  0.14861 , -0.26726 ,\n",
       "       -0.56262 ,  0.63843 , -0.54153 ,  0.36537 ,  0.20545 , -0.16604 ,\n",
       "        0.72434 ,  0.29961 , -0.42501 , -0.35932 , -0.089288,  0.48752 ,\n",
       "       -1.0927  ,  0.88818 ,  0.89941 , -0.7541  , -0.35492 , -0.76396 ,\n",
       "        0.27468 ,  0.2757  , -0.48152 , -0.41399 ,  0.64489 ,  1.148   ,\n",
       "       -0.29131 , -2.9387  , -0.83162 ,  0.95586 ,  1.1623  , -0.42502 ,\n",
       "        0.15486 ,  2.2326  , -0.31339 , -0.030228,  0.79802 , -0.41302 ,\n",
       "        0.72885 ,  0.7296  , -0.31909 ,  0.8956  ,  0.34625 ,  0.2923  ,\n",
       "        0.40056 ,  0.78985 , -0.43999 ,  0.24698 , -0.46548 ,  0.055886,\n",
       "       -0.62603 , -0.036487, -0.65429 ,  0.10563 ,  0.17435 ,  0.35466 ,\n",
       "       -1.9403  , -0.022502, -0.7302  , -0.63042 , -0.032799, -0.43953 ,\n",
       "       -0.07239 , -0.44875 , -0.074689, -0.14426 ,  0.19252 ,  0.27108 ,\n",
       "        0.20325 , -0.068109,  0.017651,  0.06455 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_layer_weights[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19916 , -0.049702,  0.24579 , -0.32281 ,  0.89768 , -0.1278  ,\n",
       "       -0.49506 ,  0.20814 , -0.20046 , -0.20604 ,  0.038292, -0.67277 ,\n",
       "       -0.12689 , -0.18766 , -0.10277 ,  0.73128 ,  0.82408 ,  0.087288,\n",
       "        0.69255 ,  1.3107  ,  0.49113 , -0.38097 ,  0.24338 , -0.27813 ,\n",
       "        0.62506 ,  0.35978 ,  0.42041 , -0.24529 ,  0.14861 , -0.26726 ,\n",
       "       -0.56262 ,  0.63843 , -0.54153 ,  0.36537 ,  0.20545 , -0.16604 ,\n",
       "        0.72434 ,  0.29961 , -0.42501 , -0.35932 , -0.089288,  0.48752 ,\n",
       "       -1.0927  ,  0.88818 ,  0.89941 , -0.7541  , -0.35492 , -0.76396 ,\n",
       "        0.27468 ,  0.2757  , -0.48152 , -0.41399 ,  0.64489 ,  1.148   ,\n",
       "       -0.29131 , -2.9387  , -0.83162 ,  0.95586 ,  1.1623  , -0.42502 ,\n",
       "        0.15486 ,  2.2326  , -0.31339 , -0.030228,  0.79802 , -0.41302 ,\n",
       "        0.72885 ,  0.7296  , -0.31909 ,  0.8956  ,  0.34625 ,  0.2923  ,\n",
       "        0.40056 ,  0.78985 , -0.43999 ,  0.24698 , -0.46548 ,  0.055886,\n",
       "       -0.62603 , -0.036487, -0.65429 ,  0.10563 ,  0.17435 ,  0.35466 ,\n",
       "       -1.9403  , -0.022502, -0.7302  , -0.63042 , -0.032799, -0.43953 ,\n",
       "       -0.07239 , -0.44875 , -0.074689, -0.14426 ,  0.19252 ,  0.27108 ,\n",
       "        0.20325 , -0.068109,  0.017651,  0.06455 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['film']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a GRU with pre-trained vectors.  Same issues as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(model1.history.history['loss'])\n",
    "plt.plot(model2.history.history['loss'])\n",
    "plt.plot(model3.history.history['loss'])\n",
    "plt.legend([\"RNN\",\"LSTM\",\"GRU\"])\n",
    "plt.title(\"Loss curves for various architectures\")\n",
    "#plt.text(0.0,0.45,\"Fig 1: A measure of RMS loss versus Epoch for several RNN Architectures \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 1: A RMS loss versus epoch for 3 different architectures.  Note how I can't spell and 2 models are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'red', 'crow', 'flies', 'north', 'at', 'dawn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
